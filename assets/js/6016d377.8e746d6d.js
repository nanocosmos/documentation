"use strict";(self.webpackChunknanocosomos_documentation=self.webpackChunknanocosomos_documentation||[]).push([[4553],{15680:(e,t,n)=>{n.d(t,{xA:()=>g,yg:()=>y});var a=n(96540);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function o(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,a,r=function(e,t){if(null==e)return{};var n,a,r={},i=Object.keys(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var s=a.createContext({}),p=function(e){var t=a.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):o(o({},t),e)),n},g=function(e){var t=p(e.components);return a.createElement(s.Provider,{value:t},e.children)},d="mdxType",m={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},u=a.forwardRef((function(e,t){var n=e.components,r=e.mdxType,i=e.originalType,s=e.parentName,g=l(e,["components","mdxType","originalType","parentName"]),d=p(n),u=r,y=d["".concat(s,".").concat(u)]||d[u]||m[u]||i;return n?a.createElement(y,o(o({ref:t},g),{},{components:n})):a.createElement(y,o({ref:t},g))}));function y(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var i=n.length,o=new Array(i);o[0]=u;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l[d]="string"==typeof e?e:r,o[1]=l;for(var p=2;p<i;p++)o[p]=n[p];return a.createElement.apply(null,o)}return a.createElement.apply(null,n)}u.displayName="MDXCreateElement"},21944:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>s,contentTitle:()=>o,default:()=>m,frontMatter:()=>i,metadata:()=>l,toc:()=>p});var a=n(58168),r=(n(96540),n(15680));const i={id:"nanostream_ios_sdk",title:"nanoStream SDK for iOS",sidebar_label:"SDK for iOS"},o=void 0,l={unversionedId:"nanostream/ios/nanostream_ios_sdk",id:"nanostream/ios/nanostream_ios_sdk",title:"nanoStream SDK for iOS",description:"The documentation is updated for nanoStream SDK v.4.10.0.0 and Xcode 11.",source:"@site/docs/nanostream/ios/nanostream_ios_sdk.md",sourceDirName:"nanostream/ios",slug:"/nanostream/ios/nanostream_ios_sdk",permalink:"/docs/nanostream/ios/nanostream_ios_sdk",draft:!1,tags:[],version:"current",lastUpdatedAt:1711640284,formattedLastUpdatedAt:"Mar 28, 2024",frontMatter:{id:"nanostream_ios_sdk",title:"nanoStream SDK for iOS",sidebar_label:"SDK for iOS"},sidebar:"nanoStream Apps & SDKs",previous:{title:"Getting started",permalink:"/docs/nanostream/ios/nanostream-ios-step-by-step"},next:{title:"bintu.live client SDK",permalink:"/docs/nanostream/ios/bintu_ios_sdk"}},s={},p=[{value:"Purpose",id:"purpose",level:2},{value:"Requirements",id:"requirements",level:2},{value:"Getting Started",id:"getting-started",level:2},{value:"Preparation",id:"preparation",level:3},{value:"Check library version",id:"check-library-version",level:3},{value:"Initialize the library for broadcasting",id:"initialize-the-library-for-broadcasting",level:3},{value:"Start a stream",id:"start-a-stream",level:3},{value:"Stop a running stream",id:"stop-a-running-stream",level:3},{value:"Live Playback",id:"live-playback",level:2},{value:"Initialize the library for playback",id:"initialize-the-library-for-playback",level:3},{value:"Start playback",id:"start-playback",level:3},{value:"Advanced Settings/Usage",id:"advanced-settingsusage",level:2},{value:"Meaning of Events and Errors (nanostreamEvent)",id:"meaning-of-events-and-errors-nanostreamevent",level:3},{value:"Switch Camera",id:"switch-camera",level:3},{value:"Orientation",id:"orientation",level:3},{value:"Stream Type",id:"stream-type",level:3},{value:"Zoom",id:"zoom",level:3},{value:"Server Authentication",id:"server-authentication",level:3},{value:"Local Recording",id:"local-recording",level:3},{value:"Cropping",id:"cropping",level:3},{value:"Adaptive Bitrate",id:"adaptive-bitrate",level:3},{value:"Statistics during streaming",id:"statistics-during-streaming",level:3},{value:"Measuring the available bandwidth",id:"measuring-the-available-bandwidth",level:3},{value:"Snaphot from the current stream",id:"snaphot-from-the-current-stream",level:3},{value:"Overlay/Watermark",id:"overlaywatermark",level:3},{value:"initWithSession",id:"initwithsession",level:3},{value:"Specific use cases",id:"specific-use-cases",level:2},{value:"Stream from a GoPro",id:"stream-from-a-gopro",level:3},{value:"Stream from a Drone",id:"stream-from-a-drone",level:3},{value:"Possible Issues",id:"possible-issues",level:2},{value:"General",id:"general",level:3},{value:"Compiler/Linker",id:"compilerlinker",level:3},{value:"libstdc++",id:"libstdc",level:4},{value:"Undefined Symbols for Parrot &amp; DJI",id:"undefined-symbols-for-parrot--dji",level:4},{value:"Duplicate Symbols",id:"duplicate-symbols",level:4},{value:"Breakpoints",id:"breakpoints",level:3},{value:"Crashes",id:"crashes",level:3},{value:"CALayerGetDelegate / CALayerGetSuperlayer / Other CALayer",id:"calayergetdelegate--calayergetsuperlayer--other-calayer",level:4},{value:"Logging Information",id:"logging-information",level:2},{value:"Crash Logs",id:"crash-logs",level:2}],g={toc:p},d="wrapper";function m(e){let{components:t,...i}=e;return(0,r.yg)(d,(0,a.A)({},g,i,{components:t,mdxType:"MDXLayout"}),(0,r.yg)("p",null,"The documentation is updated for nanoStream SDK v.4.10.0.0 and Xcode 11."),(0,r.yg)("h2",{id:"purpose"},"Purpose"),(0,r.yg)("p",null,"This documentation is about the nanoStream Live Video Streaming SDK for iOS and can be used by software developers to integrate nanoStream Live Video Encoding into custom apps."),(0,r.yg)("h2",{id:"requirements"},"Requirements"),(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},"Recommended:",(0,r.yg)("ul",{parentName:"li"},(0,r.yg)("li",{parentName:"ul"},"Apple Mac with macOS 10.15 and Xcode 11"),(0,r.yg)("li",{parentName:"ul"},"Apple iPhone/iPad with iOS 13"))),(0,r.yg)("li",{parentName:"ul"},"Minimal:",(0,r.yg)("ul",{parentName:"li"},(0,r.yg)("li",{parentName:"ul"},"Apple Mac with OS X 10.9 or higher and Xcode 6 or higher"),(0,r.yg)("li",{parentName:"ul"},"Apple iPhone/iPad with iOS 8 or higher")))),(0,r.yg)("h2",{id:"getting-started"},"Getting Started"),(0,r.yg)("h3",{id:"preparation"},"Preparation"),(0,r.yg)("p",null,"Add the library ",(0,r.yg)("strong",{parentName:"p"},(0,r.yg)("inlineCode",{parentName:"strong"},"libnanostreamAVC.a"))," as a dependency (Link Binary With Libraries) to your project."),(0,r.yg)("p",null,"Further required dependencies:"),(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"libc++.tbd")),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"Accelerate.framework")),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"SystemConfiguration.framework")),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"VideoToolbox.framework")," (link as Optional, not as Required)"),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"AudioToolbox.framework"))),(0,r.yg)("p",null,"The old nanoStream SDK and Xcode versions may additionaly require:"),(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"libc++.dylib")," (instead of ",(0,r.yg)("inlineCode",{parentName:"li"},"libc++.tbd"),")"),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"libstdc++.dylib")),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"AVFoundation.framework")),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"CoreGraphics.framework")),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"CoreMedia.framework")),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"CoreVideo.framework")),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"Foundation.framework"))),(0,r.yg)("p",null,"Include the header ",(0,r.yg)("inlineCode",{parentName:"p"},"libnanostreamAVC.h")," in your source code."),(0,r.yg)("h3",{id:"check-library-version"},"Check library version"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-objc"},"int version = [nanostreamAVC getVersion];\nif (version != NANOSTREAM_AVC_VERSION) {\n    // Handle header and library version mismatch\n}\n")),(0,r.yg)("h3",{id:"initialize-the-library-for-broadcasting"},"Initialize the library for broadcasting"),(0,r.yg)("p",null,"For a complete running sample, see LiveEncoder and LiveStream apps included in the SDK package."),(0,r.yg)("p",null,"Implement the interface ",(0,r.yg)("inlineCode",{parentName:"p"},"nanostreamEventListener")," in your class:"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-objc"},'@interface SampleLiveViewController : UIViewController <nanostreamEventListener>\n\n...\n\n@property (nonatomic, strong) nanostreamAVC *nAVC;\n@property (nonatomic, strong) IBOutlet UIView *previewView;\n\n...\n\n@end\n\n@implementation SampleLiveViewController\n\n...\n\n- (void)nanostreamEventHandlerWithType:(nanostreamEvent)type andLevel:(int)level andDescription:(NSString *)description {\n    switch (type) {\n            case StreamStarted:\n                  break;\n            case StreamStopped:\n                  break;\n            case StreamError:\n                  NSLog(@"nanostreamEventHandlerWithType:StreamError: %@", description);\n                  break;\n            case StreamErrorConnect:\n                  NSLog(@"nanostreamEventHandlerWithType:StreamErrorConnect: %@", description);\n                  break;\n            case StreamConnectionStatus:\n                  NSLog(@"nanostreamEventHandlerWithType:RtmpConnectionStatus: %@", description);\n                  break;\n            case StreamSettingCropModeNotSupported:\n                NSLog(@"nanostreamEventHandlerWithType:StreamSettingCropModeNotSupported: %@", description);\n                      break;\n            case StreamSettingLocalRecordingCropModeNotSupported:\n                NSLog(@"nanostreamEventHandlerWithType:StreamSettingLocalRecordingCropModeNotSupported: %@", description);\n                      break;        \n            case GeneralError:\n                  break;\n            default:\n                  break;\n      }\n}\n\n...\n\n@end\n')),(0,r.yg)("p",null,"Configure the settings object for the library:"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-objc"},'nanostreamAVCSettings *nAVCSettings = [[nanostreamAVCSettings alloc] init];\n\n// Set the RTMP URL, you want to stream to\n[nAVCSettings setUrl:@"rtmp://localhost/live"];\n[nAVCSettings setStreamId:@"myStream"];\n\n// Set the video settings\n[nAVCSettings setVideoResolution:Resolution640x480];\n[nAVCSettings setVideoBitrate:512];\n[nAVCSettings setKeyFramerate:60];\n[nAVCSettings setOrientation:AVCaptureVideoOrientationLandscapeRight];\n[nAVCSettings setCropMode:NoCrop];\n[nAVCSettings setFramerate:30];\n[nAVCSettings setH264Level:Baseline30];\n\n// Set the audio settings\n[nAVCSettings setInitialVolume:1.0];\n[nAVCSettings setAudioMonoStereo:Stereo];\n[nAVCSettings setAudioSamplerate:44100.0f];\n')),(0,r.yg)("p",null,"Then the library itself can be initialized:"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-objc"},'// nAVC is a property of the controller class in this example\nself.nAVC = [[nanostreamAVC alloc] initWithSettings:nAVCSettings\n                                      eventListener:self];\n\n// Set the license key (required for streaming)\n[self.nAVC setLicense:@"nlic:1.2:LiveEnc:1.1:LvApp=1.....288"];\n\n// Implement the nanostreamEventListener protocol method \n// to display a preview in the previewView\n- (void)didUpdatePreviewLayer:(CALayer*)layer {\n\n    // UI View is modified, main queue required\n    dispatch_async(dispatch_get_main_queue(), ^{\n        if (self.previewView.layer.sublayers.count > 0) {\n            self.previewView.layer.sublayers = nil;\n        }\n        layer.bounds = CGRectZero;\n        [layer setFrame:self.previewView.bounds];\n        [(AVCaptureVideoPreviewLayer*)layer setVideoGravity:AVLayerVideoGravityResizeAspectFill];\n        [self.previewView.layer addSublayer:layer];\n    });\n}\n')),(0,r.yg)("h3",{id:"start-a-stream"},"Start a stream"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-objc"},"// Start broadcast asynchronously with completion handler\n[self.nAVC start:^(NSXError error) {\n    dispatch_async(dispatch_get_main_queue(), ^{\n        if (error == NSXErrorNone) {\n            // Handle succesful stream start\n        } else {\n            // Handle failure\n        }\n    });\n}];\n")),(0,r.yg)("h3",{id:"stop-a-running-stream"},"Stop a running stream"),(0,r.yg)("p",null,"If the parameter ",(0,r.yg)("inlineCode",{parentName:"p"},"blocking")," of the stop method is set to ",(0,r.yg)("inlineCode",{parentName:"p"},"YES"),", all the remaining data (to this moment) will be sent before stopping the stream.\nIf set to ",(0,r.yg)("inlineCode",{parentName:"p"},"NO"),", the stream will stop immediately, discarding the remaining data."),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-objc"},"    // Stop broadcast asynchronously with completion handler\n    [self.nAVC stop:YES withCompletion:^{\n      // Use the main queue to update UI\n      dispatch_async(dispatch_get_main_queue(), ^{\n        // Handle stream stop\n      }\n    }];\n")),(0,r.yg)("h2",{id:"live-playback"},"Live Playback"),(0,r.yg)("p",null,"nanoStream supports live playback from ",(0,r.yg)("inlineCode",{parentName:"p"},"RTMP")," sources.\nFor a complete running sample, see LivePlayer and LiveStream apps included in the SDK package."),(0,r.yg)("h3",{id:"initialize-the-library-for-playback"},"Initialize the library for playback"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-objc"},"self.session = [[RtmpSourceCaptureSession alloc] initWithLogLevel:LogLevelMinimal];\nself.session.delegate = self;\n    \n[self.session setLicense:myLicenseKey];\n    \n[self.session setUrl:self.rtmpServerURL];\n[self.session setStreamId:self.rtmpStreamName];\n\n...\n\n// Implement the RtmpSourceCaptureSessionStatusListener protocol method \n// to display a playback in the playerView\n- (void)didUpdateDisplayLayer:(CALayer *)layer {\n\n    // UI View is modified, main queue required\n    dispatch_async(dispatch_get_main_queue(), ^{\n        if (self.playerView.layer.sublayers == nil) {\n            [self.playerView.layer addSublayer:layer];\n        } else {\n            // Replace a current display layer with the new one\n            for (CALayer* subLayer in [self.playerView.layer sublayers]) {\n                if ([subLayer isKindOfClass:AVSampleBufferDisplayLayer.class]) {\n                    [self.playerView.layer replaceSublayer:subLayer with:layer];\n                }\n            }\n        }\n\n        layer.bounds = CGRectZero;\n        [layer setFrame:self.playerView.bounds];\n        [(AVSampleBufferDisplayLayer *)layer setVideoGravity:AVLayerVideoGravityResizeAspect];\n\n        [self.session didUpdateDisplayLayerFinished];\n    });\n}\n")),(0,r.yg)("h3",{id:"start-playback"},"Start playback"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-objc"},"[self.session start:^{\n    // Hanlde playback start\n}];\n")),(0,r.yg)("h2",{id:"advanced-settingsusage"},"Advanced Settings/Usage"),(0,r.yg)("h3",{id:"meaning-of-events-and-errors-nanostreamevent"},"Meaning of Events and Errors (nanostreamEvent)"),(0,r.yg)("table",null,(0,r.yg)("thead",{parentName:"table"},(0,r.yg)("tr",{parentName:"thead"},(0,r.yg)("th",{parentName:"tr",align:null},"Event"),(0,r.yg)("th",{parentName:"tr",align:null},"Meaning"))),(0,r.yg)("tbody",{parentName:"table"},(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("strong",{parentName:"td"},(0,r.yg)("inlineCode",{parentName:"strong"},"StreamStarted/StreamStopped"))),(0,r.yg)("td",{parentName:"tr",align:null},"notification that a stream was successfully started/stopped")),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("strong",{parentName:"td"},(0,r.yg)("inlineCode",{parentName:"strong"},"StreamConnectionStatus"))),(0,r.yg)("td",{parentName:"tr",align:null},"indicates the connection status: connected, disconnected or reconnecting")))),(0,r.yg)("br",null),(0,r.yg)("table",null,(0,r.yg)("thead",{parentName:"table"},(0,r.yg)("tr",{parentName:"thead"},(0,r.yg)("th",{parentName:"tr",align:null},"Error"),(0,r.yg)("th",{parentName:"tr",align:null},"Meaning"))),(0,r.yg)("tbody",{parentName:"table"},(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("strong",{parentName:"td"},(0,r.yg)("inlineCode",{parentName:"strong"},"StreamSettingCropModeNotSupported"))),(0,r.yg)("td",{parentName:"tr",align:null},"a larger input resolution is required for the crop mode")),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("strong",{parentName:"td"},(0,r.yg)("inlineCode",{parentName:"strong"},"StreamSettingLocalRecordingCropModeNotSupported"))),(0,r.yg)("td",{parentName:"tr",align:null},"a larger input resolution is required for the local recording crop mode")),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("strong",{parentName:"td"},(0,r.yg)("inlineCode",{parentName:"strong"},"StreamErrorConnect"))),(0,r.yg)("td",{parentName:"tr",align:null},"the connection to the server could not be established, server might no be reachable or there are network problems")),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("strong",{parentName:"td"},(0,r.yg)("inlineCode",{parentName:"strong"},"StreamErrorConnectSSL"))),(0,r.yg)("td",{parentName:"tr",align:null},"the connection process failed when trying to setup SSL, maybe the server certificates could not be verified")),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("strong",{parentName:"td"},(0,r.yg)("inlineCode",{parentName:"strong"},"StreamError"))),(0,r.yg)("td",{parentName:"tr",align:null},"an error occurred while initializing the stream, or during the stream, used license might be invalid or expired")),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("strong",{parentName:"td"},(0,r.yg)("inlineCode",{parentName:"strong"},"GeneralError"))),(0,r.yg)("td",{parentName:"tr",align:null},"a general error")))),(0,r.yg)("h3",{id:"switch-camera"},"Switch Camera"),(0,r.yg)("p",null,"The camera (front/back) can be switched during preview and broadcast, with the method:"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-objc"},"- (bool)useFrontCamera:(bool)value;   // Returns true if switch was successful\n")),(0,r.yg)("p",null,"It is also possible to select the desired camera directly, when initializing the library, by using the property ",(0,r.yg)("inlineCode",{parentName:"p"},"frontCamera")," of the ",(0,r.yg)("inlineCode",{parentName:"p"},"nanostreamAVCSettings")," object:"),(0,r.yg)("h3",{id:"orientation"},"Orientation"),(0,r.yg)("p",null,"The orientation of the stream can be set to portrait or landscape with the property ",(0,r.yg)("inlineCode",{parentName:"p"},"orientation")," of the settings object."),(0,r.yg)("p",null,"As of version 4.4.0.6 the orientation can also be changed after the initialization with the property ",(0,r.yg)("inlineCode",{parentName:"p"},"orientation")," of the ",(0,r.yg)("inlineCode",{parentName:"p"},"nanostreamAVC")," object itself."),(0,r.yg)("blockquote",null,(0,r.yg)("p",{parentName:"blockquote"},(0,r.yg)("strong",{parentName:"p"},"Important:")," "),(0,r.yg)("p",{parentName:"blockquote"},"The orientation change will only affect the stream, but not the preview. The orientation for the preview has to be managed on the application level. This can be achieved by using e.g. ",(0,r.yg)("a",{parentName:"p",href:"https://developer.apple.com/documentation/coregraphics/cgaffinetransform"},(0,r.yg)("inlineCode",{parentName:"a"},"CGAffineTransformMakeRotation")," "),".")),(0,r.yg)("h3",{id:"stream-type"},"Stream Type"),(0,r.yg)("p",null,"The SDK supports different streaming modes:"),(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},"Video and Audio"),(0,r.yg)("li",{parentName:"ul"},"Video only"),(0,r.yg)("li",{parentName:"ul"},"Audio only")),(0,r.yg)("p",null,"You can configure the mode with the property ",(0,r.yg)("inlineCode",{parentName:"p"},"streamType")," of the settings object."),(0,r.yg)("h3",{id:"zoom"},"Zoom"),(0,r.yg)("p",null,"The following properties and methods are available for zooming:"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-objc"},"/**\n *  maxZoomFactor\n *\n *  Returns the maximum zoom factor that can be set on the current device.\n *  This includes the zoom via upscaling.\n *\n *  @return Maximum Zoom factor that can be set on the current device\n */\n- (CGFloat)maxZoomFactor;\n")),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-objc"},"/**\n *  Newer devices can use an almost-optical zoom, using the bigger sensor sizes.\n *  Setting the zoom factor to a value smaller than this value uses a lossless\n *  zoom.\n *\n *  @return Max Zoom factor that can be set without using upscaling.\n */\n- (CGFloat)maxZoomFactorWithoutUpscaling;\n")),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-objc"},"/**\n *  Sets the zoom factor for the camera. Available from iOS 7.1 onwards.\n *\n *  @param factor Double between 1.0 and maxZoomFactor. Will be set for the \n *  current capture device. Will be reset when the device changes (camera rotation)\n *  Zoom factor will be applied with a ramp function, which results in a smooth\n *  transition to the given factor. If a value smaller than \n *  maxZoomFactorWithoutUpscaling is set, the zoom will be lossless.\n *\n *  @return YES if zoom was set. NO if not.\n */\n- (BOOL)setZoomFactor:(CGFloat)factor;\n")),(0,r.yg)("p",null,"Zooming is also demonstrated in the sample project ",(0,r.yg)("inlineCode",{parentName:"p"},"LiveStream"),", included in the SDK package."),(0,r.yg)("h3",{id:"server-authentication"},"Server Authentication"),(0,r.yg)("p",null,"In case authentication is required, the credentials can be set with the method:"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-objc"},"- (void)setAuthentication:(NSString*)user withPassword:(NSString*)password;\n")),(0,r.yg)("p",null,"The method has to be invoked before a stream is started."),(0,r.yg)("p",null,"For example:"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-objc"},'// Set up nAVC object\n\n...\n\n[nAVC setAuthentication:@"MyUser" withPassword:@"MyPassword"];\n\n...\n\n// Start the stream\n')),(0,r.yg)("h3",{id:"local-recording"},"Local Recording"),(0,r.yg)("p",null,"It is possible to make a local copy of the stream, on the iOS device.\nThis is an extra feature and needs to be unlocked by the license - the license should contain the string ",(0,r.yg)("inlineCode",{parentName:"p"},"MP4=2"),"."),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-objc"},'NSString *homeDirectory = [NSHomeDirectory() stringByAppendingPathComponent:@"Documents"];\nNSDateFormatter *dateFormatter = [[NSDateFormatter alloc] init];\n[dateFormatter setDateFormat:@"yyyy-MM-dd_HH-mm-ss"];\nNSString *locStr = [homeDirectory stringByAppendingPathComponent:[[dateFormatter stringFromDate:[NSDate date]] stringByAppendingString:@".mp4"]];\n\n[nAVCSettings setLocalRecordingMode:AVCRecordingModeDoubleAtLeastOneMbit];\n[nAVCSettings setLocalRecordingPath:locStr];\n')),(0,r.yg)("p",null,"The following recording modes are available:"),(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("strong",{parentName:"li"},(0,r.yg)("inlineCode",{parentName:"strong"},"AVCRecordingModeStartBitrate")),": uses the video bitrate set with ",(0,r.yg)("inlineCode",{parentName:"li"},"nanostreamAVCSettings")),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("strong",{parentName:"li"},(0,r.yg)("inlineCode",{parentName:"strong"},"AVCRecordingModeDoubleAtLeastOneMbit")),": uses double the video bitrate, but at least ",(0,r.yg)("inlineCode",{parentName:"li"},"1Mbps")),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("strong",{parentName:"li"},(0,r.yg)("inlineCode",{parentName:"strong"},"AVCRecordingMode720p2Mbit")),": independent of the set video bitrate, always uses ",(0,r.yg)("inlineCode",{parentName:"li"},"2Mbps")," and a resolution of ",(0,r.yg)("inlineCode",{parentName:"li"},"1280x720")),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("strong",{parentName:"li"},(0,r.yg)("inlineCode",{parentName:"strong"},"AVCRecordingMode720p4Mbit")),": independent of the set video bitrate, always uses ",(0,r.yg)("inlineCode",{parentName:"li"},"4Mbps")," and a resolution of ",(0,r.yg)("inlineCode",{parentName:"li"},"1280x720")),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("strong",{parentName:"li"},(0,r.yg)("inlineCode",{parentName:"strong"},"AVCRecordingMode1080p4Mbit")),": independent of the set video bitrate, always uses ",(0,r.yg)("inlineCode",{parentName:"li"},"4Mbps")," and a resolution of ",(0,r.yg)("inlineCode",{parentName:"li"},"1920x1080")),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("strong",{parentName:"li"},(0,r.yg)("inlineCode",{parentName:"strong"},"AVCRecordingMode1080p6Mbit")),": independent of the set video bitrate, always uses ",(0,r.yg)("inlineCode",{parentName:"li"},"6Mbps")," and a resolution of ",(0,r.yg)("inlineCode",{parentName:"li"},"1920x1080")),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("strong",{parentName:"li"},(0,r.yg)("inlineCode",{parentName:"strong"},"AVCRecordingMode1080p8Mbit")),": independent of the set video bitrate, always uses ",(0,r.yg)("inlineCode",{parentName:"li"},"8Mbps")," and a resolution of ",(0,r.yg)("inlineCode",{parentName:"li"},"1920x1080"))),(0,r.yg)("p",null,"The bitrate for the recording remains constant during a stream. The adaptive bitrate mechanism only influences the bitrate for the stream, but not the bitrate for the recording."),(0,r.yg)("p",null,"The bitrate for the recording also depends on the video material. If there is a lot of movement in the video the bitrate will be higher than for recordings with little to no movement."),(0,r.yg)("h3",{id:"cropping"},"Cropping"),(0,r.yg)("p",null,"Both, the stream and the local recording can be transformed to a different format than the input from the camera."),(0,r.yg)("p",null,"The following example shows how to crop the format to ",(0,r.yg)("inlineCode",{parentName:"p"},"16:9"),"."),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-objc"},"// Set crop mode for the stream\n[nAVCSettings setCropMode:CropTo16By9];\n\n// Set crop mode for the local recording\n[nAVCSettings setLocalRecordingCropMode:CropTo16By9];\n")),(0,r.yg)("h3",{id:"adaptive-bitrate"},"Adaptive Bitrate"),(0,r.yg)("p",null,"By using the Adaptive Bitrate Control (",(0,r.yg)("em",{parentName:"p"},"ABC"),") the stream will automatically adjust to changes of the bandwidth."),(0,r.yg)("p",null,"There are two modes available:"),(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("p",{parentName:"li"},(0,r.yg)("strong",{parentName:"p"},(0,r.yg)("inlineCode",{parentName:"strong"},"AdaptiveBitrateControlModeQualityDegrade")),": The video quality will be changed if the bandwidth changes. For instance, if not enough bandwidth is available, the video bitrate will be decreased, which in turn degrades the video quality.")),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("p",{parentName:"li"},(0,r.yg)("strong",{parentName:"p"},(0,r.yg)("inlineCode",{parentName:"strong"},"AdaptiveBitrateControlModeFrameDrop")),": Low bandwidth is compensated by decreasing the framerate (FPS), but maintaining the video qualtiy."))),(0,r.yg)("p",null,"Make sure to set the ABC settings before a stream is started."),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-objc"},"[self.nAVC setAdaptiveBitrateControlMode:AdaptiveBitrateControlModeQualityDegrade];\n        \nAdaptiveBitrateControlSettings abr;\nabr.minimumBitrate = 100000;  // 100kb\nabr.minimumFramerate = 15;\nabr.maxPercentBitrateChange = 50;  // If the bitrate drops to less than 50% of the previous bitrate, all buffered data will be discarded\n    \n[self.nAVC setAdaptiveBitrateControlSettings:abr];\n")),(0,r.yg)("p",null,"Possible properties:"),(0,r.yg)("table",null,(0,r.yg)("thead",{parentName:"table"},(0,r.yg)("tr",{parentName:"thead"},(0,r.yg)("th",{parentName:"tr",align:null},"property"),(0,r.yg)("th",{parentName:"tr",align:null},"default values"),(0,r.yg)("th",{parentName:"tr",align:null},"range of values"),(0,r.yg)("th",{parentName:"tr",align:null},"optional"))),(0,r.yg)("tbody",{parentName:"table"},(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("strong",{parentName:"td"},(0,r.yg)("inlineCode",{parentName:"strong"},"minimumBitrate"))),(0,r.yg)("td",{parentName:"tr",align:null},"5000 (50 kb)"),(0,r.yg)("td",{parentName:"tr",align:null},"50000 - 10 000 000"),(0,r.yg)("td",{parentName:"tr",align:null},"YES")),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("strong",{parentName:"td"},(0,r.yg)("inlineCode",{parentName:"strong"},"minimumFramerate"))),(0,r.yg)("td",{parentName:"tr",align:null},"15 (fps)"),(0,r.yg)("td",{parentName:"tr",align:null},"5 - 60"),(0,r.yg)("td",{parentName:"tr",align:null},"YES")),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("strong",{parentName:"td"},(0,r.yg)("inlineCode",{parentName:"strong"},"maxPercentBitrateChange"))),(0,r.yg)("td",{parentName:"tr",align:null},"50 (%)"),(0,r.yg)("td",{parentName:"tr",align:null},"0 - 100"),(0,r.yg)("td",{parentName:"tr",align:null},"YES")))),(0,r.yg)("h3",{id:"statistics-during-streaming"},"Statistics during streaming"),(0,r.yg)("p",null,"You can track the current bandwidth with the delegate method:"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-objc"},"- (void)nanoStreamStatisticsHandlerWithOutputBitrate:(long long)oBitrate\n                                         andFillness:(double)fillness\n                              andCurrentVideoBitrate:(long long)vBitrate\n                            andCurrentVideoFramerate:(double)vFramerate\n")),(0,r.yg)("p",null,"This method will be invoked every second."),(0,r.yg)("p",null,"Meaning of the parameters:"),(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("strong",{parentName:"li"},(0,r.yg)("inlineCode",{parentName:"strong"},"oBitrate")),": the measured outgoing bitrate for the stream - this is an estimation and probably not 100% accurate"),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("strong",{parentName:"li"},(0,r.yg)("inlineCode",{parentName:"strong"},"fillness")),": the fillness, in percent, of the buffer used for outgoing packages"),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("strong",{parentName:"li"},(0,r.yg)("inlineCode",{parentName:"strong"},"vBitrate")),": the currently used bitrate for video"),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("strong",{parentName:"li"},(0,r.yg)("inlineCode",{parentName:"strong"},"vFramerate")),": the currently used framerate for video")),(0,r.yg)("table",null,(0,r.yg)("thead",{parentName:"table"},(0,r.yg)("tr",{parentName:"thead"},(0,r.yg)("th",{parentName:"tr",align:"left"},"property"),(0,r.yg)("th",{parentName:"tr",align:"left"},"default values"),(0,r.yg)("th",{parentName:"tr",align:"left"},"range of values"),(0,r.yg)("th",{parentName:"tr",align:"left"},"optional"))),(0,r.yg)("tbody",{parentName:"table"},(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:"left"},(0,r.yg)("strong",{parentName:"td"},(0,r.yg)("inlineCode",{parentName:"strong"},"minimumBitrate"))),(0,r.yg)("td",{parentName:"tr",align:"left"},"5000 (50 kb)"),(0,r.yg)("td",{parentName:"tr",align:"left"},"50000 - 10 000 000"),(0,r.yg)("td",{parentName:"tr",align:"left"},"YES")),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:"left"},(0,r.yg)("strong",{parentName:"td"},(0,r.yg)("inlineCode",{parentName:"strong"},"minimumFramerate"))),(0,r.yg)("td",{parentName:"tr",align:"left"},"15 (fps)"),(0,r.yg)("td",{parentName:"tr",align:"left"},"5 - 60"),(0,r.yg)("td",{parentName:"tr",align:"left"},"YES")),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:"left"},(0,r.yg)("strong",{parentName:"td"},(0,r.yg)("inlineCode",{parentName:"strong"},"maxPercentBitrateChange"))),(0,r.yg)("td",{parentName:"tr",align:"left"},"50 (%)"),(0,r.yg)("td",{parentName:"tr",align:"left"},"0 - 100"),(0,r.yg)("td",{parentName:"tr",align:"left"},"YES")))),(0,r.yg)("h3",{id:"measuring-the-available-bandwidth"},"Measuring the available bandwidth"),(0,r.yg)("p",null,"For measuring the available bandwidth you can use the method ",(0,r.yg)("inlineCode",{parentName:"p"},"runBandwidthCheck"),". After the check finished, the result can be used to set the bitrate for the ",(0,r.yg)("inlineCode",{parentName:"p"},"nanostreamAVC")," object."),(0,r.yg)("p",null,"The check measures the bandwidth by running a test stream to the server."),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-objc"},'NSXBandwidthCheckSettings *bwSettings = [[NSXBandwidthCheckSettings alloc] init];\n\n// The URL settings are identical to the URL settings for the nanostreamAVCSettings\n// for testing the bandwidth it is advised to use the same server you want to stream to\n// you might want to use a stream id different from the stream id for the actual stream, \n// to distinguish between a bandwidth check and a real stream\nbwSettings.url = @"rtmp://localhost/live";\nbwSettings.streamId = @"bwcheck";\n\n// The maxium bitrate that should be tested - if this value is lower than the actual bandwidth, \n// the result will be similar to the maximum bitrate\nbwSettings.maxBitrate = 5000000; // 5Mb\n    \n[self.nAVC runBandwidthCheck:bwSettings withCompletionBlock:^(NSXBandwidthCheckResult *measuredBandwidth) {\n    NSLog(@"measuredBandwidth: avg=%i, median=%i, min=%i, max=%i, runTimeMs=%i", (int)measuredBandwidth.avgBitrate, (int)measuredBandwidth.medianBitrate, (int)measuredBandwidth.minBitrate, (int)measuredBandwidth.maxBitrate, (int)measuredBandwidth.runTimeMs);\n}];\n')),(0,r.yg)("p",null,"The default run time is 10 seconds. The run time can be changed with the property ",(0,r.yg)("inlineCode",{parentName:"p"},"runTime"),".\nIf the bandwidth check should be stopped before it finished on itself, the method ",(0,r.yg)("inlineCode",{parentName:"p"},"stopBandwidthCheck")," can be used. This will force the bandwidth check to stop and return the result based on the collected information up to this point."),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-objc"},"[self.nAVC stopBandwidthCheck]; // Stop bandwidth check if still running\n")),(0,r.yg)("p",null,"The result of the bandwidth check can be used as bitrate setting for library object. At the moment it is not possible to change the video bitrate after the initialization of the library object, thus the object need to be re-initialized. (This will change in future releases.)"),(0,r.yg)("h3",{id:"snaphot-from-the-current-stream"},"Snaphot from the current stream"),(0,r.yg)("p",null,"To get a snaphot (image) of the current preview/stream, the method ",(0,r.yg)("inlineCode",{parentName:"p"},"grabStillImageWithCompletionBlock")," can be used."),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-objc"},"[self.nAVC grabStillImageWithCompletionBlock:^(UIImage *image, NSError *error) {\n      // Do something with the image\n}];\n")),(0,r.yg)("h3",{id:"overlaywatermark"},"Overlay/Watermark"),(0,r.yg)("p",null,"It is possible to use an overlay (image, text, or both) for a stream. Notice that the CPU usage will be increased slightly when an overlay is used.\nThis is an extra feature and needs to be unlocked by the license - the license should contain the string ",(0,r.yg)("inlineCode",{parentName:"p"},"OVL=1"),"."),(0,r.yg)("p",null,"The easiest way to use an overlay is to use the class ",(0,r.yg)("inlineCode",{parentName:"p"},"AVCFullImageOverlay"),":"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-objc"},'UIImage *overlayImg = [UIImage imageNamed:@"button"];  // Uses an image from the bundle resources, named "button"\n        \nUIGraphicsBeginImageContextWithOptions(CGSizeMake(640, 480), NO, 1.0);  // Assuming the video resolution is set to "Resolution640x480"\n[overlayImg drawInRect:CGRectMake(200, 200, 240, 80) blendMode:kCGBlendModeNormal alpha:0.5];\nUIFont *font = [UIFont boldSystemFontOfSize:20];\n[[UIColor whiteColor] set];\nNSString *text = @"Watermark";\n[text drawInRect:CGRectMake(200, 300, 100, 50) withFont:font];\nUIImage *finalOverlayImage = UIGraphicsGetImageFromCurrentImageContext();\nUIGraphicsEndImageContext();\n        \n[self.nAVC setOverlay:[[AVCFullImageOverlay alloc] initWithImage:finalOverlayImage]];\n')),(0,r.yg)("p",null,"Notice that the final output resolution can be different, if an option like cropping is used.\nIn this case it is better to implement your own overlay class, which is shown in the following example:"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-objc"},"@interface NSXWatermark () : NSObject `<\u200bAVCOverlay>`\u200b\n    \n@property (assign) AVCOverlayRawBuffer buffer;\n@property (strong) __attribute__((NSObject)) CFDataRef imageDataForBuffer;\n    \n@end\n    \n    \n@implementation NSXWatermark\n    \n@synthesize overlayBoundingRect;\n@synthesize imageSize;\n    \n    \n- (AVCOverlayRawBuffer)overlayImageWithStreamTime:(NSTimeInterval)time {\n      if (self.buffer.buffer == NULL) {\n          UIImage *image = [self generateWatermarkWithSize:self.imageSize inBoundingRect:self.overlayBoundingRect];\n          if (image) {\n              [self setupBufferWithImage:image];\n          }\n      }\n        \n      return self.buffer;\n}\n    \n- (UIImage *)generateWatermarkWithSize:(CGSize)size inBoundingRect:(CGRect)boundingRect {\n      UIImage *overlayImage;\n      CGFloat padding = 10.0;\n        \n      // Permanent Overlay\n      CGRect permRealRect;\n      UIImage *permanentImage = self.permanentOverlayImage;\n      if (permanentImage) {\n          CGSize overlaySize = permanentImage.size;\n            \n          CGFloat height = size.height / 3;\n          if (overlaySize.height > height) {\n              overlaySize.width = height;\n              overlaySize.height = height;\n          }\n            \n          CGFloat boundingMaxX = boundingRect.origin.x + boundingRect.size.width;\n          CGFloat boundingMaxY = boundingRect.origin.y + boundingRect.size.height;\n            \n            \n          CGRect permOverlayRect = CGRectMake(boundingMaxX - overlaySize.width/2, boundingMaxY - overlaySize.height/2, overlaySize.width/2, overlaySize.height/2);\n            \n          permRealRect =  AVMakeRectWithAspectRatioInsideRect(permanentImage.size, permOverlayRect);\n            \n          permRealRect.origin.y -= padding;\n          permRealRect.origin.x -= padding;\n      }\n        \n      UIGraphicsBeginImageContext(size);\n        \n      if (permanentImage) {\n          [permanentImage drawInRect:permRealRect];\n      }\n        \n      overlayImage = UIGraphicsGetImageFromCurrentImageContext();\n        \n      UIGraphicsEndImageContext();\n        \n      return overlayImage;\n}\n    \n- (void)setupBufferWithImage:(UIImage *)image {\n      CGImageRef rawPic = [image CGImage];\n      CFDataRef data = [NSXWatermark copyDataFromUIImage:rawPic];\n      AVCOverlayRawBuffer buf = [NSXWatermark makeBufferFromData:data andImage:rawPic];\n        \n      self.buffer = buf;\n      self.imageDataForBuffer = data;\n        \n      CFRelease(data);\n}\n    \n+ (CFDataRef)copyDataFromUIImage:(CGImageRef)image {\n      CGDataProviderRef inProvider = CGImageGetDataProvider(image);\n      CFDataRef inBitmapData = CGDataProviderCopyData(inProvider);\n      return inBitmapData;\n}\n    \n+ (AVCOverlayRawBuffer)makeBufferFromData:(CFDataRef)inBitmapData andImage:(CGImageRef)rawPic {\n      AVCOverlayRawBuffer rawBuf;\n        \n      size_t inBitmapDataBytesPerRow = CGImageGetBytesPerRow(rawPic);\n        \n      UInt8 *buffer = (UInt8*)CFDataGetBytePtr(inBitmapData);\n        \n      rawBuf.buffer = buffer;\n      rawBuf.bytesPerRow = (int)inBitmapDataBytesPerRow;\n      rawBuf.bufferType = AVCOverlayBufferTypeBGRA;\n        eturn rawBuf;\n}\n    \n@end\n")),(0,r.yg)("p",null,"If you want to use a dynamic overlay, e.g. a scoreboard, you can modify the above class (",(0,r.yg)("inlineCode",{parentName:"p"},"NSXWatermark"),") like so:"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-objc"},"...\n\n@property (strong) NSString *overlayURL; // Needs to be set\n@property (nonatomic, strong) NSDate *date;\n@property (nonatomic, strong) UIImage *watermarkHttpImage;\n\n...\n\n- (AVCOverlayRawBuffer)overlayImageWithStreamTime:(NSTimeInterval)time {\n    if (!self.date) {\n        self.date = [NSDate date];\n    }\n    NSTimeInterval timeInterval = time <= 0.6 ? 0.6 : time;\n    NSTimeInterval passedTime = [[NSDate date] timeIntervalSinceDate:self.date];\n    if (passedTime > timeInterval && self.overlayURL) {\n        [self loadHTTPImageAsync];\n        UIImage *image = [self generateWatermarkWithSize:self.imageSize inBoundingRect:self.overlayBoundingRect];\n        if (image) {\n            [self setupBufferWithImage:image];\n            self.date = [NSDate date];\n        }\n    } else if (self.buffer.buffer == NULL) {\n        UIImage *image = [self generateWatermarkWithSize:self.imageSize inBoundingRect:self.overlayBoundingRect];\n        if (image) {\n            [self setupBufferWithImage:image];\n            self.date = [NSDate date];\n        }\n    }\n\n    return self.buffer;\n}\n\n- (UIImage *)generateWatermarkWithSize:(CGSize)size inBoundingRect:(CGRect)boundingRect {\n\n    ...\n\n    // Dynamic Overlay\n    CGRect dynRealRect;\n    UIImage *dynamicWatermarkImage = self.watermarkHttpImage;\n    if (dynamicWatermarkImage) {\n        CGSize overlaySize = dynamicWatermarkImage.size;\n\n        CGFloat height = size.height / 3;\n        if (overlaySize.height > height) {\n            overlaySize.width = height;\n            overlaySize.height = height;\n        }\n\n        CGRect dynOverlayRect = CGRectMake(0, 0, overlaySize.width, overlaySize.height);\n\n        dynRealRect =  AVMakeRectWithAspectRatioInsideRect(dynamicWatermarkImage.size, dynOverlayRect);\n\n        dynRealRect.origin.y = padding;\n        dynRealRect.origin.x = padding;\n    }\n\n    ...\n\n}\n\n- (void)loadHTTPImageAsync {\n    dispatch_async(dispatch_get_global_queue(0,0), ^{\n        NSData *data = [[NSData alloc] initWithContentsOfURL:[NSURL URLWithString:self.overlayURL]];\n        if ( data == nil )\n            return;\n        dispatch_async(dispatch_get_main_queue(), ^{\n            self.watermarkHttpImage = [UIImage imageWithData:data];\n        });\n    });\n}\n\n...\n")),(0,r.yg)("h3",{id:"initwithsession"},"initWithSession"),(0,r.yg)("p",null,"Instead of letting the SDK manage the video and audio input, you can also do that yourself. This is helpful to supply video and audio samples which are not coming from the standard input devices. Or to modify video and/or audio samples before they are used for the stream."),(0,r.yg)("p",null,"The SDK provides a separate init method ",(0,r.yg)("inlineCode",{parentName:"p"},"initWithSession"),"."),(0,r.yg)("p",null,"An example for a custom capture session, which supplies ",(0,r.yg)("inlineCode",{parentName:"p"},"CVPixelBufferRef"),"'s to the SDK:"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-objc"},"@interface CustomCaptureSession : AVCaptureSession\n\n@property (nonatomic, strong) AVCaptureVideoDataOutput* myVideoOutput;\n@property (nonatomic, strong) AVCaptureAudioDataOutput* myAudioOutput;\n\n...\n\n@end\n\n@implementation CustomCaptureSession\n\n- (void) addOutput:(AVCaptureOutput *)output {\n    if ([output isKindOfClass:[AVCaptureVideoDataOutput class]]) {\n        self.myVideoOutput = (AVCaptureVideoDataOutput*)output;\n    } else if([output isKindOfClass:[AVCaptureAudioDataOutput class]]) {\n        *self.myAudioOutput = (AVCaptureAudioDataOutput*)output; * if you want to use a custom audio capture device\n        [super addOutput:output]; // Uses the standard microphone of the iOS device\n    }\n}\n\n- (void) addInput:(AVCaptureInput *)input {\n    [super addInput:input]; // This is required, because nanostreamAVC checks the available inputs\n}\n\n- (void) startRunning {\n    [super startRunning];\n}\n\n// This method has to be called periodically - e.g. with CADisplayLink\n- (void) supplyCMSampleBufferRef {\n    CVPixelBufferRef buffer = [self getCVPixelBufferRef]; // Get the CVPixelBufferRef from somewhere\n\n    CMSampleBufferRef newSampleBuffer = NULL;\n    CMSampleTimingInfo timingInfo = kCMTimingInfoInvalid;\n    timingInfo.duration = CMTimeMake(33, 1000);    // Assuming 30fps, change if otherwise\n    timingInfo.decodeTimeStamp = CMTimeMake(ts, 1000);    // Timestamp information required\n    timingInfo.presentationTimeStamp = timingInfo.decodeTimeStamp;\n\n    CMVideoFormatDescriptionRef videoInfo = NULL;\n    CMVideoFormatDescriptionCreateForImageBuffer(NULL, buffer, &videoInfo);\n\n    CMSampleBufferCreateForImageBuffer(kCFAllocatorDefault, buffer, true, NULL, NULL, videoInfo, &timingInfo, &newSampleBuffer);\n\n    // The following line submits the new CMSampleBufferRef to the nanostreamAVC lib\n    [self.myVideoOutput.sampleBufferDelegate captureOutput:self.myVideoOutput didOutputSampleBuffer:newSampleBuffer fromConnection:nil];\n\n    CFRelease(videoInfo);\n    CFRelease(buffer);\n    CFRelease(newSampleBuffer);\n}\n\n@end\n\n// You need to use initWithSession for nanostreamAVC to use your custom session\n\n...\n\nsession = [[CustomCaptureSession alloc] init];\n\n// Use microphone from iOS device as audio source\nNSError *error;\nAVCaptureInput *audioInput = [AVCaptureDeviceInput deviceInputWithDevice:[AVCaptureDevice defaultDeviceWithMediaType:AVMediaTypeAudio] error:&error];\n\nif (audioInput != nil) {\n    [session addInput:audioInput]; // If the stream should be video only, don't add an audioInput\n}\n\n[session startRunning];\n\n...\n\nself.stream = [[nanostreamAVC alloc] initWithSession:session settings:nAVCSettings eventListener:self];\n\n...\n")),(0,r.yg)("h2",{id:"specific-use-cases"},"Specific use cases"),(0,r.yg)("h3",{id:"stream-from-a-gopro"},"Stream from a GoPro"),(0,r.yg)("p",null,"Instead of using the build-in camera and microphone of an iOS device, it is also possible to use external devices like a GoPro camera."),(0,r.yg)("p",null,"To use this feature you will need a special build of our SDK. Please contact us for more information."),(0,r.yg)("p",null,"The following pseudo code shows how to use a GoPro camera as source:"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-objc"},'#import "nanostreamAVCExternals.h"\n\n@interface YourLiveViewController `<nanostreamEventListener, ExtendedCaptureSessionStatusListener>`\n\n...\n\n@property (nonatomic, strong) nanostreamAVC *stream;\n@property (strong) AVCaptureSession *session;\n@property (nonatomic, strong) IBOutlet UIView *cameraView;\n\n...\n\n@end\n\n@implementation YourLiveViewController\n\n...\n\nif (self.cameraSource == GoProHero3) {\n    self.session = [[HLSCaptureSession alloc] initWithPreview:cameraView andStatusListener:self andUDP:NO];\n\n}\nelse if (self.cameraSource == GoProHero4) {\n    self.session = [[HLSCaptureSession alloc] initWithPreview:cameraView andStatusListener:self andUDP:YES];\n}\n\n[self.session startRunning];\n\n// Set up settings for nanostreamAVC\n\n...\n\nself.stream = [[nanostreamAVC alloc] initWithSession:self.session\n                                            settings:nAVCSettings\n                                       eventListener:self];\n\n...\n\n@end\n')),(0,r.yg)("h3",{id:"stream-from-a-drone"},"Stream from a Drone"),(0,r.yg)("p",null,"Get in touch with us to learn how to send a live stream from a Drone (Parrot Bebop or DJI)"),(0,r.yg)("h2",{id:"possible-issues"},"Possible Issues"),(0,r.yg)("h3",{id:"general"},"General"),(0,r.yg)("p",null,"For older versions of the sdk, without support for ",(0,r.yg)("inlineCode",{parentName:"p"},"arm64"),", architecture in ",(0,r.yg)("inlineCode",{parentName:"p"},"Xcode")," has to be set to ",(0,r.yg)("inlineCode",{parentName:"p"},"armv7")," and/or ",(0,r.yg)("inlineCode",{parentName:"p"},"armv7s"),". This works also for newer iOS-Devcies like iPhone 5s."),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"This is not required for newer sdk versions"),", which also support ",(0,r.yg)("inlineCode",{parentName:"p"},"arm64"),"."),(0,r.yg)("h3",{id:"compilerlinker"},"Compiler/Linker"),(0,r.yg)("h4",{id:"libstdc"},"libstdc++"),(0,r.yg)("p",null,"If there are linker errors with ",(0,r.yg)("inlineCode",{parentName:"p"},'std::: "symbol(s) not found for architecture"'),", make sure that you added the libraries ",(0,r.yg)("inlineCode",{parentName:"p"},"libstdc++.dylib")," and ",(0,r.yg)("inlineCode",{parentName:"p"},"libc++.dylib")," to your project."),(0,r.yg)("p",null,'Due to a bug in Xcode, depending on the selected Base SDK and deployment target, there might be still linker errors regarding "std". In this case you need to add a specific version of the libstdc++ to your project, e.g.: libstdc++-6.0.9.dylib instead of libstdc++.dylib'),(0,r.yg)("h4",{id:"undefined-symbols-for-parrot--dji"},"Undefined Symbols for Parrot & DJI"),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"The following part is only relevant for SDK versions from 3.3.x to 4.1.x."),"\nAs of version 4.2.x the drone dependencies are removed from the standard SDK package."),(0,r.yg)("p",null,"It might be possible that there are linker errors for the classes"),(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("p",{parentName:"li"},(0,r.yg)("inlineCode",{parentName:"p"},"ParrotBebopCaptureSession")," or")),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("p",{parentName:"li"},(0,r.yg)("inlineCode",{parentName:"p"},"DJIPhantom2CaptureSession")))),(0,r.yg)("p",null,"Generally, if the Parrot & DJI extensions are not used, the symbols should be stripped automatically by ",(0,r.yg)("inlineCode",{parentName:"p"},"Xcode")," and you do not need to link the frameworks.\nHowever this is not the case when the linker flag -",(0,r.yg)("inlineCode",{parentName:"p"},"ObjC")," is used in the app project. This causes the linker to load all symbols included in all linked object files (including the Parrot & DJI symbols). This prevents the automatic stripping. "),(0,r.yg)("p",null,"To use our library without Parrot & DJI, either remove the -",(0,r.yg)("inlineCode",{parentName:"p"},"ObjC")," linker flag from the project or replace the -",(0,r.yg)("inlineCode",{parentName:"p"},"ObjC")," linker flag with the ",(0,r.yg)("inlineCode",{parentName:"p"},"-force_load")," flag for each library that you want to use. Do not use ",(0,r.yg)("inlineCode",{parentName:"p"},"-force_load")," with ",(0,r.yg)("inlineCode",{parentName:"p"},"libnanostreamAVC.a"),". "),(0,r.yg)("p",null,"For examples ",(0,r.yg)("a",{parentName:"p",href:"http://stackoverflow.com/questions/11254269/using-the-force-load-linker-flag-with-restkit-ios"},"see")),(0,r.yg)("h4",{id:"duplicate-symbols"},"Duplicate Symbols"),(0,r.yg)("p",null,'If there are duplicate symbol errors, check the "Other Linker Flags" setting. Select a target, go to "Build Settings" and type into the search field "Other Linker Flags".'),(0,r.yg)("p",null,"Check if the linker flag ",(0,r.yg)("inlineCode",{parentName:"p"},"-all_load")," is used. Either remove the flag or if you have to use it, consider using the flag ",(0,r.yg)("inlineCode",{parentName:"p"},"-force_load")," or ",(0,r.yg)("inlineCode",{parentName:"p"},"-ObjC")," instead."),(0,r.yg)("p",null,(0,r.yg)("a",{parentName:"p",href:"http://stackoverflow.com/questions/3354864/xcode-project-target-settings-syntax-for-linker-flag-force-load-on-iphone"},'See also here for more information on the "-force',"_",'load" flag')),(0,r.yg)("p",null,(0,r.yg)("a",{parentName:"p",href:"https://developer.apple.com/library/mac/qa/qa1490/_index.html"},'Check also this information if you have a "selector not recognized" runtime exception')),(0,r.yg)("h3",{id:"breakpoints"},"Breakpoints"),(0,r.yg)("p",null,"If you debug your application, it is possible that breakpoints are being hit due to internal exceptions. Exceptions on the SDK level are handled in the SDK and do not affect the workflow of your application."),(0,r.yg)("p",null,"You can prevent the breakpoint from pausing the workflow of your application, if you use the right settings for the breakpoint.\nThe default setting is most likely that every exception causes a break.\nTo change that, use the settings from the following screenshot:"),(0,r.yg)("p",null,(0,r.yg)("img",{alt:"Screenshot",src:n(20641).A,width:"696",height:"202"})),(0,r.yg)("p",null,"This way only ",(0,r.yg)("inlineCode",{parentName:"p"},"Objective-C exceptions")," will be catched and ",(0,r.yg)("inlineCode",{parentName:"p"},"C++ exceptions")," will be ignored."),(0,r.yg)("h3",{id:"crashes"},"Crashes"),(0,r.yg)("h4",{id:"calayergetdelegate--calayergetsuperlayer--other-calayer"},"CALayerGetDelegate / CALayerGetSuperlayer / Other CALayer"),(0,r.yg)("p",null,"If there are crashes occurring in your app that include above symbols in the stack trace and are otherwise not obvious, check to see if you added a subviews to the preview view. The ",(0,r.yg)("inlineCode",{parentName:"p"},"UIView")," instance that is passed to "),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-objc"},"- [RtmpSourceCaptureSession initWithPreview:andStatusListener:andLogLevel:]\n")),(0,r.yg)("p",null,"and "),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-objc"},"- [nanostreamAVC initWithSettings:uiPreview:eventListener:]\n")),(0,r.yg)("p",null,"cannot contain any subviews (UIButtons or otherwise)."),(0,r.yg)("h2",{id:"logging-information"},"Logging Information"),(0,r.yg)("p",null,"If you encounter a problem with the ",(0,r.yg)("inlineCode",{parentName:"p"},"nanostreamAVC")," library and you want to report the problem, log files will help us to comprehend the problem."),(0,r.yg)("p",null,"Please use the following steps to create the log files:"),(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("strong",{parentName:"li"},"for the encoder (nanostreamAVC)"),': enable logging for the library with the method "SetLogLevel", use LogLevelCustomSupport (if not available use LogLevelVerbose): ')),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-objc"},'[self.nAVC SetLogLevel:LogLevelCustomSupport];  // set the log level before the method "start" is invoked\n')),(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("strong",{parentName:"li"},"for the player (RtmpSourceCaptureSession)"),": the log level has to be set in the constructor: ")),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-objc"},"self.session = [[RtmpSourceCaptureSession alloc] initWithPreview:self.playerView andStatusListener:self andLogLevel:LogLevelCustomSupport];\n")),(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("p",{parentName:"li"},"try to reproduce the problem")),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("p",{parentName:"li"},"download the app container (for your app) from the iOS device with ",(0,r.yg)("inlineCode",{parentName:"p"},"Xcode"),", as explained ",(0,r.yg)("a",{parentName:"p",href:"http://help.apple.com/xcode/mac/8.0/#/dev816c242e1"},"here"))),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("p",{parentName:"li"},"in Finder right click on the downloaded container and select ",(0,r.yg)("inlineCode",{parentName:"p"},"Show Package Contents"))),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("p",{parentName:"li"},"send us ",(0,r.yg)("strong",{parentName:"p"},"all")," log files located (in the container) in the folder ",(0,r.yg)("inlineCode",{parentName:"p"},"/AppData/Library/Caches/Logs/")))),(0,r.yg)("h2",{id:"crash-logs"},"Crash Logs"),(0,r.yg)("p",null,"If you encounter a crash, please send us the crash log as explained in the following steps:"),(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},"Plug in the device and open ",(0,r.yg)("inlineCode",{parentName:"li"},"Xcode")),(0,r.yg)("li",{parentName:"ul"},"Choose ",(0,r.yg)("inlineCode",{parentName:"li"},"Window --\x3e Devices")," from the menu bar"),(0,r.yg)("li",{parentName:"ul"},"Under the DEVICES section in the left column, choose the device"),(0,r.yg)("li",{parentName:"ul"},"To see the device console, click the up-triangle at the bottom left of the right hand panel"),(0,r.yg)("li",{parentName:"ul"},"Click the down arrow on the bottom right to save the console as a file"),(0,r.yg)("li",{parentName:"ul"},"To see crash logs, select the View Device Logs button under the Device Information section on the right hand panel"),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("strong",{parentName:"li"},"Wait until the Scanning Process is complete"),", this can take a while (there should be a text at the top of the Window, something like \"Scanning '...crash'...\")"),(0,r.yg)("li",{parentName:"ul"},"Use the column ",(0,r.yg)("inlineCode",{parentName:"li"},"Date/Time")," to order the crashes after dates."),(0,r.yg)("li",{parentName:"ul"},"Find your app in the Process column and select the Crash log to see the contents."),(0,r.yg)("li",{parentName:"ul"},"To save a crash log, right click the entry on the left column and choose ",(0,r.yg)("inlineCode",{parentName:"li"},"Export Log"))),(0,r.yg)("p",null,"Taken from ",(0,r.yg)("a",{parentName:"p",href:"https://developer.apple.com/library/ios/qa/qa1747/_index.html"},"here")))}m.isMDXComponent=!0},20641:(e,t,n)=>{n.d(t,{A:()=>a});const a=n.p+"assets/images/screenshot_exception_breakpoint-a189a542733dd31469bc16cbc08cc91d.png"}}]);